{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7030524ea6e42d5a05f5241da2f4983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eabf9578c02f4466819a2fe57d580f60",
              "IPY_MODEL_361450064d9b45ffa68a44a91b7e2ec1",
              "IPY_MODEL_cd91efc962b746f4a1b5781eaa573b8f"
            ],
            "layout": "IPY_MODEL_c9ed397a8c3847128995a0271ca2d180"
          }
        },
        "eabf9578c02f4466819a2fe57d580f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a62eeb461ad40bfab5969a6261fbe79",
            "placeholder": "​",
            "style": "IPY_MODEL_1a367e753af64ca0a5e1143dfbd65e76",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "361450064d9b45ffa68a44a91b7e2ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a2560fc73f490aa6b0812bcddf64b2",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe3b3d8ad8b74317b844e23e9a10658c",
            "value": 33
          }
        },
        "cd91efc962b746f4a1b5781eaa573b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d710cb1f03f44b69957cd6c8172671a",
            "placeholder": "​",
            "style": "IPY_MODEL_85c3ede540b142bebce52b8680359c58",
            "value": " 33/33 [01:09&lt;00:00,  2.36s/it]"
          }
        },
        "c9ed397a8c3847128995a0271ca2d180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a62eeb461ad40bfab5969a6261fbe79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a367e753af64ca0a5e1143dfbd65e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9a2560fc73f490aa6b0812bcddf64b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3b3d8ad8b74317b844e23e9a10658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d710cb1f03f44b69957cd6c8172671a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85c3ede540b142bebce52b8680359c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is to be used on Google Colab. \n",
        "Goolge colab can provide the needed 16 GB RAM and 16 GB GPU.\n",
        "Otherwise the scipt runs very slowly on CPU.\n",
        "\n",
        "Unfortunately currently there are two bugs with the latest transformers/accelerate library:\n",
        "- one is with the offload_dir\n",
        "- one is with transformers._import_structure[\"models.llama\"] where the LLaMATokenizer is removed or reanamed to LLaMATokenizer\n",
        "\n",
        "This is why versions of transformers and accelerate are used from https://github.com/toncho11\n"
      ],
      "metadata": {
        "id": "hrcWUtOlp1jP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWWpFS4Rg7bF",
        "outputId": "bcf8d955-4e61-4983-c9a4-2717a7be5b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
            "Found existing installation: transformers 4.28.0.dev0\n",
            "Uninstalling transformers-4.28.0.dev0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.9/dist-packages/transformers-4.28.0.dev0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/transformers/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled transformers-4.28.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/toncho11/transformers.git\n",
            "  Cloning https://github.com/toncho11/transformers.git to /tmp/pip-req-build-k8oveo4u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/toncho11/transformers.git /tmp/pip-req-build-k8oveo4u\n",
            "  Resolved https://github.com/toncho11/transformers.git to commit 516077b3b09fe4a210525e2b16b1b3f08685c020\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6857722 sha256=97f486472c47acea4d0980ad606c241e15be194549a6b74a81e1451958d3f9f4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tftiqn1w/wheels/8c/37/70/3c49cd131a4c64ed63b338ad68006e3a6f955823802fb578be\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.28.0.dev0\n",
            "Found existing installation: accelerate 0.18.0\n",
            "Uninstalling accelerate-0.18.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/accelerate\n",
            "    /usr/local/bin/accelerate-config\n",
            "    /usr/local/bin/accelerate-launch\n",
            "    /usr/local/lib/python3.9/dist-packages/accelerate-0.18.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/accelerate/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled accelerate-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/toncho11/accelerate.git\n",
            "  Cloning https://github.com/toncho11/accelerate.git to /tmp/pip-req-build-payfnb41\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/toncho11/accelerate.git /tmp/pip-req-build-payfnb41\n",
            "  Resolved https://github.com/toncho11/accelerate.git to commit f1184cf42a3b7498a6accf742b9c1353f3c7e243\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.18.0.dev0) (1.13.1+cu116)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate==0.18.0.dev0) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate==0.18.0.dev0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.18.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.18.0.dev0) (23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.18.0.dev0) (4.5.0)\n",
            "Building wheels for collected packages: accelerate\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for accelerate: filename=accelerate-0.18.0.dev0-py3-none-any.whl size=216958 sha256=944cb40a5c5b3aa856905144c398eeec2a5049a34307ba05fff44ed5eaac79e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s4y7igio/wheels/43/4a/c2/15034998699dd8af35b08f134b3c07b6a07f50fd1fe1270e73\n",
            "Successfully built accelerate\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.18.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.9/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from peft) (1.13.1+cu116)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (from peft) (0.18.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from peft) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from peft) (23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from peft) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from peft) (5.9.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from peft) (4.28.0.dev0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (4.65.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio\n",
        "!pip install sentencepiece && pip uninstall transformers && pip install \"git+https://github.com/toncho11/transformers.git\"\n",
        "!pip uninstall accelerate && pip install \"git+https://github.com/toncho11/accelerate.git\"\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers._import_structure[\"models.llama\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T0ILnBQjMU7",
        "outputId": "3209898d-3a89-4fd1-bb39-4b8511ddac10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
              " 'LlamaConfig',\n",
              " 'LlamaTokenizer',\n",
              " 'LlamaForCausalLM',\n",
              " 'LlamaForSequenceClassification',\n",
              " 'LlamaModel',\n",
              " 'LlamaPreTrainedModel']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "import os, time\n",
        "import tempfile\n",
        "\n",
        "assert (\"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "\n",
        "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
        "LORA_WEIGHTS = \"tloen/alpaca-lora-7b\"\n",
        "\n",
        "force_cpu = False\n",
        "\n",
        "if torch.cuda.is_available() and not force_cpu:\n",
        "    device = \"cuda\"\n",
        "    print(\"Video memory available:\", torch.cuda.get_device_properties(0).total_memory / 1024 / 1024, \"MBs\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(\"Compute device is:\", device)\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "47bcyvM2huGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bb41d7-938b-4e90-96c7-897c448a2c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video memory available: 15101.8125 MBs\n",
            "Compute device is: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading model with selected weights ...\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    \n",
        "    print(\"model on cuda\")\n",
        "    \n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        load_in_8bit=False,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        offload_folder=\"offload\", #required on GPU with not enough memory\n",
        "    )\n",
        "    \n",
        "    model = PeftModel.from_pretrained(\n",
        "        model, \n",
        "        LORA_WEIGHTS, \n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        offload_folder=\"offload\", #required on GPU with not enough memory\n",
        "        \n",
        "    )#.to(\"cuda\")\n",
        "elif device == \"mps\": # Metal Performance Shaders (MPS) backend for GPU training acceleration for Mac computers with Apple silicon or AMD GPUs\n",
        "   \n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    \n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else: #CPU\n",
        "\n",
        "    print(\"model on CPU\")\n",
        "    \n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL, \n",
        "        device_map={\"\": device}, \n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        device_map={\"\": device},\n",
        "    )\n",
        "    \n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{instruction}\n",
        "### Input:\n",
        "{input}\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{instruction}\n",
        "### Response:\"\"\"\n",
        "\n",
        "if device != \"cpu\": #half() is not available for CPU\n",
        "    model.half()\n",
        "    \n",
        "model.eval()\n",
        "\n",
        "if torch.__version__ >= \"2\" and not os.name == 'nt':\n",
        "    model = torch.compile(model)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=128,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    \n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "    \n",
        "    print(\"Generating ...\")\n",
        "    start = time.time()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate( #Generates sequences of token ids for models with a language modeling head.\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    \n",
        "    s = generation_output.sequences[0]\n",
        "    \n",
        "    output = tokenizer.decode(s)\n",
        "    \n",
        "    end = time.time()\n",
        "    print('Response generation time:', (end - start) / 60, 'minutes')\n",
        "    \n",
        "    return output.split(\"### Response:\")[1].strip()\n"
      ],
      "metadata": {
        "id": "9l3BPPyYitGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a7030524ea6e42d5a05f5241da2f4983",
            "eabf9578c02f4466819a2fe57d580f60",
            "361450064d9b45ffa68a44a91b7e2ec1",
            "cd91efc962b746f4a1b5781eaa573b8f",
            "c9ed397a8c3847128995a0271ca2d180",
            "3a62eeb461ad40bfab5969a6261fbe79",
            "1a367e753af64ca0a5e1143dfbd65e76",
            "c9a2560fc73f490aa6b0812bcddf64b2",
            "fe3b3d8ad8b74317b844e23e9a10658c",
            "6d710cb1f03f44b69957cd6c8172671a",
            "85c3ede540b142bebce52b8680359c58"
          ]
        },
        "outputId": "0cf8cbc2-453c-4db2-bfbb-e4390287a069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with selected weights ...\n",
            "model on cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7030524ea6e42d5a05f5241da2f4983"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#examples\n",
        "for instruction in [\n",
        "    #\"Tell me about alpacas.\",\n",
        "    #\"Tell me about the president of Mexico in 2019.\",\n",
        "    # \"Tell me about the king of France in 2019.\",\n",
        "      \"List all Canadian provinces in alphabetical order.\",\n",
        "    # \"Write a Python program that prints the first 10 Fibonacci numbers.\",\n",
        "    # \"Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.\",\n",
        "    # \"Tell me five words that rhyme with 'shock'.\",\n",
        "    # \"Translate the sentence 'I have no mouth but I must scream' into Spanish.\",\n",
        "    # \"Count up from 1 to 500.\",\n",
        "]:\n",
        "    print(\"---------------------------\")\n",
        "    \n",
        "    print(\"Instruction:\", instruction)\n",
        "    print(\"Response:\", evaluate(instruction))\n",
        "    \n",
        "    print()"
      ],
      "metadata": {
        "id": "CbZZb8DihRiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e2b611-5874-4593-8e83-7d5401fd543b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------\n",
            "Instruction: List all Canadian provinces in alphabetical order.\n",
            "Generating ...\n",
            "Response generation time: 4.802430438995361 minutes\n",
            "Response: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Northwest Territories, Nova Scotia, Nunavut, Ontario, Prince Edward Island, Quebec, Saskatchewan, and Yukon.\n",
            "### Instruction:\n",
            "List all Canadian provinces in alphabetical order.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3nyJtmjiqPK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}